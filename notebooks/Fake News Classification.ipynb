{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fake News Classification.ipynb","provenance":[],"collapsed_sections":["sAoL6xZ9TWAu","-QBw_HDMdypR","QYoLDMPqd0Z9","8joNFOFcd5RS","aCabuf1kcevt","JE3RUFr8cm8c","X-QxEI6_cq9u","CFY_3_xicv1C","QWd2V31ac5ce","O2noXWW6dHDJ","SvHNpiMGdRyL","CAl49IyzdTVB"],"authorship_tag":"ABX9TyMVYvEO3phMn0UEiUtkzxnp"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"tgo50mE_VbOM","colab_type":"code","colab":{}},"source":["import os\n","from typing import Tuple\n","from pathlib import Path\n","import argparse\n","import pickle\n","import _pickle as cPickle\n","from zipfile import ZipFile\n","import string\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","tqdm.pandas()\n","from sklearn.metrics import f1_score, accuracy_score\n","from sklearn.model_selection import StratifiedKFold\n","\n","import fasttext\n","import flair\n","from flair.models import TextClassifier\n","from flair.data import Sentence\n","import allennlp\n","import classifiers as cl\n","\n","import keras as k\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","from gensim.models import KeyedVectors\n","\n","from tensorflow import keras, split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, Dense, Lambda, Reshape, Dropout, Embedding, GRU, LSTM, Bidirectional, Conv1D, GlobalMaxPool1D, concatenate\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import optimizers"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GU6G9hiCbGtz","colab_type":"text"},"source":["# Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"aHvZpjBaTQ40","colab_type":"code","colab":{}},"source":["columns =['id',\t'label'\t,'statement',\t'subject',\t'speaker', \t'job', \t'state',\n","        'party',\t'barely_true_c',\t'false_c',\t'half_true_c',\t'mostly_true_c',\n","        'pants_on_fire_c',\t'venue']\n","train_data = pd.read_table('train.tsv', names = columns)\n","train_data = train_data.drop(['barely_true_c','false_c','half_true_c','mostly_true_c','pants_on_fire_c'], axis=1)\n","#train_data[:] = train_data[:].fillna('0')\n","headers = ['subject',\t'speaker', \t'job', \t'state','party','venue']\n","for header in headers:\n","  frequent = train_data[header].str.lower().value_counts()[:15].reset_index().to_dict()['index']\n","  frequent = dict((i,k) for k,i in frequent.items())\n","  def get_venue_id(venue):\n","    if isinstance(venue, str):\n","      matched = [ven for ven in frequent if ven in venue.lower() ]\n","      if len(matched)>0:\n","        return frequent[matched[0]]\n","      else:\n","        return len(set(frequent.values())) \n","    else:\n","        return len(set(frequent.values()))\n","  train_data[header+'_id'] = train_data[header].apply(get_venue_id)\n","\n","valid_data = pd.read_table('valid.tsv', names = columns)\n","valid_data = valid_data.drop(['barely_true_c','false_c','half_true_c','mostly_true_c','pants_on_fire_c'], axis=1)\n","#valid_data[:] = valid_data[:].fillna('0')\n","for header in headers:\n","  frequent = valid_data[header].str.lower().value_counts()[:15].reset_index().to_dict()['index']\n","  frequent = dict((i,k) for k,i in frequent.items())\n","  def get_venue_id(venue):\n","    if isinstance(venue, str):\n","      matched = [ven for ven in frequent if ven in venue.lower() ]\n","      if len(matched)>0:\n","        return frequent[matched[0]]\n","      else:\n","        return len(set(frequent.values())) \n","    else:\n","        return len(set(frequent.values()))\n","  valid_data[header+'_id'] = valid_data[header].apply(get_venue_id)\n","\n","test_data = pd.read_table('test.tsv', names = columns)\n","test_data = test_data.drop(['barely_true_c','false_c','half_true_c','mostly_true_c','pants_on_fire_c'], axis=1)\n","#test_data[:] = test_data[:].fillna('0')\n","for header in headers:\n","  frequent = test_data[header].str.lower().value_counts()[:15].reset_index().to_dict()['index']\n","  frequent = dict((i,k) for k,i in frequent.items())\n","  def get_venue_id(venue):\n","    if isinstance(venue, str):\n","      matched = [ven for ven in frequent if ven in venue.lower() ]\n","      if len(matched)>0:\n","        return frequent[matched[0]]\n","      else:\n","        return len(set(frequent.values())) \n","    else:\n","        return len(set(frequent.values()))\n","    test_data[header+'_id'] = test_data[header].apply(get_venue_id)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"svPQDIRYUbdy","colab_type":"code","colab":{}},"source":["for header in headers:\n","  frequent = train_data[header].str.lower().value_counts()[:15].reset_index().to_dict()['index']\n","  frequent = dict((i,k) for k,i in frequent.items())\n","  choices = frequent.keys()\n","  header_id= []\n","  for value in range(len(train_data[header])):\n","    new, score = process.extractOne(train_data[header][value], choices,scorer=fuzz.ratio)\n","    header_id.append(frequent[new])\n","  train_data[header+'_id'] = header_id\n","\n","valid_data = pd.read_table('valid.tsv', names = columns)\n","valid_data = valid_data.drop(['barely_true_c','false_c','half_true_c','mostly_true_c','pants_on_fire_c'], axis=1)\n","valid_data[:] = valid_data[:].fillna('0')\n","for header in headers:\n","  frequent = valid_data[header].str.lower().value_counts()[:15].reset_index().to_dict()['index']\n","  frequent = dict((i,k) for k,i in frequent.items())\n","  choices = frequent.keys()\n","  header_id= []\n","  for value in range(len(valid_data[header])):\n","    new, score = process.extractOne(valid_data[header][value], choices,scorer=fuzz.ratio)\n","    header_id.append(frequent[new])\n","  valid_data[header+'_id'] = header_id\n","\n","test_data = pd.read_table('test.tsv', names = columns)\n","test_data = test_data.drop(['barely_true_c','false_c','half_true_c','mostly_true_c','pants_on_fire_c'], axis=1)\n","test_data[:] = test_data[:].fillna('0')\n","for header in headers:\n","  frequent = test_data[header].str.lower().value_counts()[:15].reset_index().to_dict()['index']\n","  frequent = dict((i,k) for k,i in frequent.items())\n","  choices = frequent.keys()\n","  header_id= []\n","  for value in range(len(test_data[header])):\n","    new, score = process.extractOne(test_data[header][value], choices,scorer=fuzz.ratio)\n","    header_id.append(frequent[new])\n","  test_data[header+'_id'] = header_id"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sAoL6xZ9TWAu","colab_type":"text"},"source":["#### Label"]},{"cell_type":"code","metadata":{"id":"waB0CFT6TYvx","colab_type":"code","colab":{}},"source":["y_label_dict = {\"pants-fire\" : 0, \"false\" : 1, \"barely-true\" : 2, \"half-true\" : 3, \"mostly-true\" : 4, \"true\" : 5}\n","train_data['output'] = train_data['label'].apply(lambda x: y_label_dict[x])\n","valid_data['output'] = valid_data['label'].apply(lambda x: y_label_dict[x])\n","test_data['output'] = test_data['label'].apply(lambda x: y_label_dict[x])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q4-gdV4edwv2","colab_type":"text"},"source":["#### Statements"]},{"cell_type":"code","metadata":{"id":"IP5xmFwta_A1","colab_type":"code","colab":{}},"source":["import pickle\n","def load_tokenizer(filename = 'tokenizer.pickle'):\n","    with open(filename, 'rb') as handle:\n","        tokenizer = pickle.load(handle)\n","    return tokenizer\n","def load_tokenizers(count=6, filename='tokenizer'):\n","    ts = []\n","    for i in range(count):\n","        ts.append(load_tokenizer(filename=filename+\"_\"+str(i)+\".pickle\"))\n","    return ts\n","cols = ['subject', 'speaker', 'job', 'state', 'party', 'venue']\n","def load_df(filename):\n","    return pd.read_pickle(filename)\n","def load_embedding_matrix(filename = 'embedding_matrix.npy'):\n","    return np.load(filename)\n","def df_to_input(df, tokenizer, max_len):\n","    return pad_sequences(tokenizer.texts_to_sequences(df.p_statement), maxlen=max_len, padding='post', truncating='post')\n","\n","def load_x(path, keywords, size):\n","  master_data = np.load(path)\n","  x = np.empty((size,))\n","  for data in master_data:\n","    if all(f in data for f in keywords):\n","      x = np.column_stack([x, master_data[data]])\n","  x = x[:,1:]\n","  if any(w in keywords for w in ['LR', 'SVM', 'FT']):\n","    x = np.array(np.column_stack([x[:,:3], np.stack(x[:,3])]), dtype=np.float)\n","  return x\n","\n","def df_to_meta_input(df, tokenizers, columns):\n","    a = np.zeros((df.shape[0], len(columns)), dtype=int)\n","    for i, col in enumerate(columns):\n","        a[:,i] = np.array(tokenizers[i].texts_to_sequences(df[col])).reshape(-1)\n","    return a"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XaZnzN1DFa5_","colab_type":"code","colab":{}},"source":["sentence_max_len = 15\n","t = load_tokenizer()\n","stmt_embedding = load_embedding_matrix()\n","df_train = load_df('df_train.pkl')\n","df_val = load_df('df_val.pkl')\n","df_test = load_df('df_test.pkl')\n","df_train = df_train.replace('', 'nan')\n","\n","stmt_train = df_to_input(df_train, t, sentence_max_len)\n","stmt_val = df_to_input(df_val, t, sentence_max_len)\n","stmt_test = df_to_input(df_test, t, sentence_max_len)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-QBw_HDMdypR","colab_type":"text"},"source":["#### DEP"]},{"cell_type":"code","metadata":{"id":"lZX6VYdcTsOR","colab_type":"code","colab":{}},"source":["dep_dict = {'punct' : 0, 'prep' : 1, 'pobj' : 2, 'compound' : 3, 'det' : 4, \n","            'nsubj' : 5, 'ROOT' : 6, 'amod' : 7, 'dobj' : 8, 'aux' : 9, \n","            'advmod' : 10, 'nummod' : 10, 'ccomp' : 10, 'conj' : 10, 'cc' : 10, \n","            'advcl' : 10, 'poss' : 10, 'mark' : 10, 'quantmod' : 10, 'relcl' : 10, \n","            'attr' : 10, 'xcomp' : 10, 'npadvmod' : 10, 'nmod' : 10, 'auxpass' : 10, \n","            'acl' : 10, 'nsubjpass' : 10, 'pcomp' : 10, 'acomp' : 10, 'neg' : 10, \n","            'appos' : 10, 'prt' : 10, '' : 10, 'expl' : 10, 'dative' : 10, \n","            'agent' : 10, 'case' : 10, 'oprd' : 10, 'csubj' : 10, 'dep' : 10, \n","            'intj' : 10, 'predet' : 10, 'parataxis' : 10, 'preconj' : 10, \n","            'meta' : 10, 'csubjpass' : 10}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NI1bYeq0d0CE","colab_type":"code","colab":{}},"source":["def get_dep_parse(statement):\n","  # doc = nlp(statement.decode('utf-8', 'ignore'))\n","  doc = nlp(statement)\n","  deplist = []\n","  for token in doc:\n","    deplist.append(dep_dict.get(token.dep_, max(dep_dict.values())))\n","  return deplist\n","\n","train_data['dep_id'] = train_data['statement'].apply(get_dep_parse)\n","valid_data['dep_id'] = valid_data['statement'].apply(get_dep_parse)\n","test_data['dep_id'] = test_data['statement'].apply(get_dep_parse)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QYoLDMPqd0Z9","colab_type":"text"},"source":["#### POS"]},{"cell_type":"code","metadata":{"id":"N75x1_OVd1kj","colab_type":"code","colab":{}},"source":["pos_dict = {'NOUN' : 0, 'VERB' : 1, 'ADP' : 2, 'PROPN' : 3, 'PUNCT' : 4, \n","            'DET' : 5, 'ADJ' : 6, 'NUM' : 7, 'ADV' : 8, 'PRON' : 9, 'X' : 9, \n","            'PART' : 9, 'SYM' : 9, 'INTJ' : 9 }"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3U6wRJvxTlyP","colab_type":"code","colab":{}},"source":["def get_pos(statement):\n","  # doc = nlp(statement.decode('utf-8', 'ignore'))\n","  doc = nlp(statement)\n","  taglist = []\n","  deplist = []\n","  for token in doc:\n","    taglist.append(pos_dict.get(token.pos_,max(pos_dict.values())))\n","    #deplist.append(token.dep_)\n","  return taglist\n","\n","train_data['pos_id'] = train_data['statement'].apply(get_pos)\n","valid_data['pos_id'] = valid_data['statement'].apply(get_pos)\n","test_data['pos_id'] = test_data['statement'].apply(get_pos)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Xk1GpQbd1-3","colab_type":"text"},"source":["#### Metadata"]},{"cell_type":"code","metadata":{"id":"77AbOCS-d46j","colab_type":"code","colab":{}},"source":["ts = load_tokenizers()\n","\n","meta_train = df_to_meta_input(df_train, ts, cols)\n","meta_val = df_to_meta_input(df_val, ts, cols)\n","meta_test = df_to_meta_input(df_test, ts, cols)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8joNFOFcd5RS","colab_type":"text"},"source":["#### Sentiment Analysis"]},{"cell_type":"markdown","metadata":{"id":"_uBU27DHvvQD","colab_type":"text"},"source":["The codes below are adopted from https://github.com/prrao87/fine-grained-sentiment"]},{"cell_type":"code","metadata":{"id":"g0qtV5tNd8Fq","colab_type":"code","colab":{}},"source":["basic = cl.Base()\n","liar_train = basic.read_data(fname='./data/liar_dataset/train-clean.txt', lower_case=True)\n","liar_valid = basic.read_data(fname='./data/liar_dataset/valid-clean.txt', lower_case=True)\n","liar_test = basic.read_data(fname='./data/liar_dataset/test-clean.txt', lower_case=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"svCSGzxei_Pm","colab_type":"text"},"source":["TextBlob"]},{"cell_type":"code","metadata":{"id":"R_ssXHQyjDnF","colab_type":"code","outputId":"893fa676-1056-46f9-961e-03f13bfa01c6","executionInfo":{"status":"ok","timestamp":1588287565534,"user_tz":-120,"elapsed":27759,"user":{"displayName":"ham dim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTz2Pi1jTnKOWioMXecZGzGRYFRgCuR4ladvqH=s64","userId":"17299942068028895662"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# TextBlob\n","model0 = cl.TextBlobSentiment()\n","df0 = model0.predict(train_file=None, test_file='./data/test.txt', lower_case=True)\n","print('TextBlob')\n","model0.accuracy(df0)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TextBlob\n","Accuracy: 22.134\n","Macro F1-score: 19.036\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VSegGpZVjIfV","colab_type":"code","colab":{}},"source":["liar_train['textblob_score'] = liar_train['text'].apply(model0.score)\n","liar_train['textblob_pred'] = pd.cut(liar_train['textblob_score'],bins=3,labels=[0, 1, 2])\n","liar_valid['textblob_score'] = liar_valid['text'].apply(model0.score)\n","liar_valid['textblob_pred'] = pd.cut(liar_valid['textblob_score'],bins=3,labels=[0, 1, 2])\n","liar_test['textblob_score'] = liar_test['text'].apply(model0.score)\n","liar_test['textblob_pred'] = pd.cut(liar_test['textblob_score'],bins=3,labels=[0, 1, 2])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lwPsnWxljMOb","colab_type":"text"},"source":["Vader"]},{"cell_type":"code","metadata":{"id":"ksNH7ma5jLU0","colab_type":"code","outputId":"b85ccddd-f243-4558-b99c-4d63135ff24e","executionInfo":{"status":"ok","timestamp":1588287570290,"user_tz":-120,"elapsed":29838,"user":{"displayName":"ham dim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTz2Pi1jTnKOWioMXecZGzGRYFRgCuR4ladvqH=s64","userId":"17299942068028895662"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# Vader\n","model1 = cl.VaderSentiment()\n","df1 = model1.predict(train_file=None, test_file='./data/test.txt', lower_case=True)\n","print('Vader')\n","model1.accuracy(df1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Vader\n","Accuracy: 38.933\n","Macro F1-score: 37.135\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z9304MPhjSjc","colab_type":"code","colab":{}},"source":["liar_train['vader_score'] = liar_train['text'].apply(model1.score)\n","liar_train['vader_pred'] = pd.cut(liar_train['vader_score'],bins=[-float('Inf'), -0.05, 0.05, float('Inf')],labels=[0, 1, 2])\n","liar_valid['vader_score'] = liar_valid['text'].apply(model1.score)\n","liar_valid['vader_pred'] = pd.cut(liar_valid['vader_score'],bins=[-float('Inf'), -0.05, 0.05, float('Inf')],labels=[0, 1, 2])\n","liar_test['vader_score'] = liar_test['text'].apply(model1.score)\n","liar_test['vader_pred'] = pd.cut(liar_test['vader_score'],bins=[-float('Inf'), -0.05, 0.05, float('Inf')],labels=[0, 1, 2])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aaVjC-qSjVhh","colab_type":"text"},"source":["Logistic Regression"]},{"cell_type":"code","metadata":{"id":"pG9CCPAojYiT","colab_type":"code","outputId":"67860e01-7e72-432d-c770-813cc785b6ac","executionInfo":{"status":"ok","timestamp":1588287582948,"user_tz":-120,"elapsed":39767,"user":{"displayName":"ham dim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTz2Pi1jTnKOWioMXecZGzGRYFRgCuR4ladvqH=s64","userId":"17299942068028895662"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["# Logistic Regression\n","model2 = cl.LogisticRegressionSentiment()\n","df2 = model2.predict(train_file='./data/train.txt', test_file='./data/test.txt', lower_case=True)\n","print('Logistic Regression')\n","model2.accuracy(df2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Logistic Regression\n","Accuracy: 65.609\n","Macro F1-score: 57.293\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r1dLzIxwjb8_","colab_type":"code","colab":{}},"source":["liar_train[['lr_prob_neg','lr_prob_neu','lr_prob_pos']] = pd.DataFrame(model2.pipeline.predict_proba(liar_train['text']))\n","liar_train['lr_pred'] = model2.pipeline.predict(liar_train['text'])\n","liar_valid[['lr_prob_neg','lr_prob_neu','lr_prob_pos']] = pd.DataFrame(model2.pipeline.predict_proba(liar_valid['text']))\n","liar_valid['lr_pred'] = model2.pipeline.predict(liar_valid['text'])\n","liar_test[['lr_prob_neg','lr_prob_neu','lr_prob_pos']] = pd.DataFrame(model2.pipeline.predict_proba(liar_test['text']))\n","liar_test['lr_pred'] = model2.pipeline.predict(liar_test['text'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WxKngE6wjhi2","colab_type":"text"},"source":["SVM"]},{"cell_type":"code","metadata":{"id":"uTRWX94ujiFw","colab_type":"code","outputId":"8d66d3cc-4bb0-45e2-d960-b145f7b666d6","executionInfo":{"status":"ok","timestamp":1588287592163,"user_tz":-120,"elapsed":46384,"user":{"displayName":"ham dim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTz2Pi1jTnKOWioMXecZGzGRYFRgCuR4ladvqH=s64","userId":"17299942068028895662"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# SVM\n","model3 = cl.SVMSentiment()\n","df3 = model3.predict(train_file='./data/train.txt', test_file='./data/test.txt', lower_case=True)\n","print('SVM')\n","model3.accuracy(df3)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["SVM\n","Accuracy: 63.086\n","Macro F1-score: 51.346\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pj81h_SPjlrB","colab_type":"code","colab":{}},"source":["liar_train[['svm_df_neg','svm_df_neu','svm_df_pos']] = pd.DataFrame(model3.pipeline.decision_function(liar_train['text']))\n","liar_train['svm_pred'] = model3.pipeline.predict(liar_train['text'])\n","liar_valid[['svm_df_neg','svm_df_neu','svm_df_pos']] = pd.DataFrame(model3.pipeline.decision_function(liar_valid['text']))\n","liar_valid['svm_pred'] = model3.pipeline.predict(liar_valid['text'])\n","liar_test[['svm_df_neg','svm_df_neu','svm_df_pos']] = pd.DataFrame(model3.pipeline.decision_function(liar_test['text']))\n","liar_test['svm_pred'] = model3.pipeline.predict(liar_test['text'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-KqbwZYxjoez","colab_type":"text"},"source":["FastText"]},{"cell_type":"code","metadata":{"id":"UoE8dYt2j0dV","colab_type":"code","colab":{}},"source":["def extract_flair(df_in, model_flair, k):\n","  outcome = pd.DataFrame(df_in['text'].apply(lambda x: model_flair.model.predict(x, k)).tolist())\n","  df_out = pd.DataFrame(columns=['flair_prob_neg','flair_prob_neu','flair_prob_pos'])\n","  for i in range(df_in.shape[0]):\n","    d = {}\n","    for j in range(k):\n","      if outcome[0][i][j] == '__label__0':\n","        d['flair_prob_neg'] = outcome[1][i][j]\n","      elif outcome[0][i][j] == '__label__1':\n","        d['flair_prob_neu'] = outcome[1][i][j]\n","      elif outcome[0][i][j] == '__label__2':\n","        d['flair_prob_pos'] = outcome[1][i][j]\n","    df_out = df_out.append(d, ignore_index=True)\n","  return df_out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PclrADLCxeaM","colab_type":"code","colab":{}},"source":["#fasttext\n","!python3 train_fasttext.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4aI-A6jdj4Jh","colab_type":"code","outputId":"e236eb42-e7df-4a8e-e72c-ef8bf2203f6b","executionInfo":{"status":"ok","timestamp":1588287631172,"user_tz":-120,"elapsed":80872,"user":{"displayName":"ham dim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTz2Pi1jTnKOWioMXecZGzGRYFRgCuR4ladvqH=s64","userId":"17299942068028895662"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["# FastText\n","model4 = cl.FastTextSentiment('./data/fasttext-model.bin')\n","df4 = model4.predict(train_file=None, test_file='./data/test.txt', lower_case=True)\n","print('FastText')\n","model4.accuracy(df4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["FastText\n","Accuracy: 58.832\n","Macro F1-score: 48.330\n"],"name":"stdout"},{"output_type":"stream","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"fctiYK56j7RS","colab_type":"code","colab":{}},"source":["liar_train[['ft_prob_neg','ft_prob_neu','ft_prob_pos']] = extract_flair(liar_train, model4, 3)\n","liar_train['ft_pred'] = liar_train['text'].apply(model4.score)\n","liar_valid[['ft_prob_neg','ft_prob_neu','ft_prob_pos']] = extract_flair(liar_valid, model4, 3)\n","liar_valid['ft_pred'] = liar_valid['text'].apply(model4.score)\n","liar_test[['ft_prob_neg','ft_prob_neu','ft_prob_pos']] = extract_flair(liar_test, model4, 3)\n","liar_test['ft_pred'] = liar_test['text'].apply(model4.score)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dy_InbSAkEAH","colab_type":"text"},"source":["Flair"]},{"cell_type":"code","metadata":{"id":"13Sd56vFxg8e","colab_type":"code","colab":{}},"source":["#flair\n","!python3 train_flair.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JeS6w6jckEn6","colab_type":"code","colab":{}},"source":["def score(model_flair, text):\n","  doc = Sentence(text)\n","  model_flair.model.predict(doc)\n","  output_class = doc.labels[0].value\n","  output_prob = doc.labels[0].score\n","  return [output_class, output_prob]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"phPNvTYwkKsu","colab_type":"text"},"source":["Flair-GloVe"]},{"cell_type":"code","metadata":{"id":"qQRvTifrkOaN","colab_type":"code","outputId":"2e939bc9-7252-4c67-f21e-201ab193e251","executionInfo":{"status":"ok","timestamp":1588287831470,"user_tz":-120,"elapsed":173441,"user":{"displayName":"ham dim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTz2Pi1jTnKOWioMXecZGzGRYFRgCuR4ladvqH=s64","userId":"17299942068028895662"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# Flair GloVe\n","model51 = cl.FlairSentiment('./data/glove/best-model.pt')\n","df51 = model51.predict(train_file=None, test_file='./data/test.txt', lower_case=True)\n","print('Flair with GloVe')\n","model51.accuracy(df51)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-04-30 23:00:58,758 loading file ./data/glove/best-model.pt\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1387/1387 [02:25<00:00,  9.55it/s]"],"name":"stderr"},{"output_type":"stream","text":["Flair with GloVe\n","Accuracy: 59.481\n","Macro F1-score: 39.407\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wKuEigpykSqT","colab_type":"code","outputId":"c377a944-745f-49f9-945b-40e1f6cd913b","executionInfo":{"status":"ok","timestamp":1588291054873,"user_tz":-120,"elapsed":3396837,"user":{"displayName":"ham dim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTz2Pi1jTnKOWioMXecZGzGRYFRgCuR4ladvqH=s64","userId":"17299942068028895662"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["liar_train['flair_glove_prob'], liar_train['flair_glove_class']  = liar_train['text'].progress_apply(lambda x: score(model51, x)[1]), liar_train['text'].progress_apply(lambda x: score(model51, x)[0])\n","liar_valid['flair_glove_prob'], liar_valid['flair_glove_class']  = liar_valid['text'].progress_apply(lambda x: score(model51, x)[1]), liar_valid['text'].progress_apply(lambda x: score(model51, x)[0])\n","liar_test['flair_glove_prob'], liar_test['flair_glove_class']  = liar_test['text'].progress_apply(lambda x: score(model51, x)[1]), liar_test['text'].progress_apply(lambda x: score(model51, x)[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 10240/10240 [21:30<00:00,  7.93it/s]\n","100%|██████████| 10240/10240 [21:26<00:00,  7.96it/s]\n","100%|██████████| 1284/1284 [02:41<00:00,  7.97it/s]\n","100%|██████████| 1284/1284 [02:41<00:00,  7.97it/s]\n","100%|██████████| 1267/1267 [02:41<00:00,  7.83it/s]\n","100%|██████████| 1267/1267 [02:42<00:00,  7.81it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"1XqmYk4IkVh6","colab_type":"text"},"source":["Flair-ELMo"]},{"cell_type":"code","metadata":{"id":"4_Ky3PBBkZBd","colab_type":"code","outputId":"106e594c-1936-4242-c4cf-f5bedd572ee0","executionInfo":{"status":"ok","timestamp":1588291216860,"user_tz":-120,"elapsed":3558815,"user":{"displayName":"ham dim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTz2Pi1jTnKOWioMXecZGzGRYFRgCuR4ladvqH=s64","userId":"17299942068028895662"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# Flair ELMo\n","model52 = cl.FlairSentiment('./data/elmo/best-model.pt')\n","df52 = model52.predict(train_file=None, test_file='./data/test.txt', lower_case=True)\n","print('Flair with ELMo')\n","model52.accuracy(df52)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-04-30 23:57:34,423 loading file ./data/elmo/best-model.pt\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1387/1387 [02:24<00:00,  9.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Flair with ELMo\n","Accuracy: 60.634\n","Macro F1-score: 38.016\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"RE1KwlJvkcll","colab_type":"code","outputId":"f27cecca-0d0b-429d-a218-07d4ea8e5d20","executionInfo":{"status":"ok","timestamp":1588294432670,"user_tz":-120,"elapsed":6774619,"user":{"displayName":"ham dim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTz2Pi1jTnKOWioMXecZGzGRYFRgCuR4ladvqH=s64","userId":"17299942068028895662"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["liar_train['flair_elmo_prob'], liar_train['flair_elmo_class']  = liar_train['text'].progress_apply(lambda x: score(model52, x)[1]), liar_train['text'].progress_apply(lambda x: score(model52, x)[0])\n","liar_valid['flair_elmo_prob'], liar_valid['flair_elmo_class']  = liar_valid['text'].progress_apply(lambda x: score(model52, x)[1]), liar_valid['text'].progress_apply(lambda x: score(model52, x)[0])\n","liar_test['flair_elmo_prob'], liar_test['flair_elmo_class']  = liar_test['text'].progress_apply(lambda x: score(model52, x)[1]), liar_test['text'].progress_apply(lambda x: score(model52, x)[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 10240/10240 [21:26<00:00,  7.96it/s]\n","100%|██████████| 10240/10240 [21:23<00:00,  7.98it/s]\n","100%|██████████| 1284/1284 [02:40<00:00,  8.01it/s]\n","100%|██████████| 1284/1284 [02:40<00:00,  7.98it/s]\n","100%|██████████| 1267/1267 [02:41<00:00,  7.83it/s]\n","100%|██████████| 1267/1267 [02:42<00:00,  7.80it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"nGZa0fE8kf1k","colab_type":"text"},"source":["Flair-BERT"]},{"cell_type":"code","metadata":{"id":"_Zfdldo1kimG","colab_type":"code","outputId":"d303c153-c63f-4941-ba5a-597fe20d545c","executionInfo":{"status":"ok","timestamp":1588296740101,"user_tz":-120,"elapsed":1390535,"user":{"displayName":"ham dim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTz2Pi1jTnKOWioMXecZGzGRYFRgCuR4ladvqH=s64","userId":"17299942068028895662"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# Flair BERT\n","model53 = cl.FlairSentiment('./data/bert/best-model.pt')\n","df53 = model53.predict(train_file=None, test_file='./data/test.txt', lower_case=True)\n","print('Flair with BERT')\n","model53.accuracy(df53)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-05-01 01:09:09,771 loading file ./data/bert/best-model.pt\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1387/1387 [23:02<00:00,  1.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Flair with BERT\n","Accuracy: 61.283\n","Macro F1-score: 41.172\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"u2xPRWhokl4q","colab_type":"code","outputId":"a317ce9a-294e-453d-8fd3-908a1fe11526","executionInfo":{"status":"ok","timestamp":1588321769254,"user_tz":-120,"elapsed":3233697,"user":{"displayName":"ham dim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTz2Pi1jTnKOWioMXecZGzGRYFRgCuR4ladvqH=s64","userId":"17299942068028895662"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["liar_train['flair_bert_prob'], liar_train['flair_bert_class']  = liar_train['text'].progress_apply(lambda x: score(model53, x)[1]), liar_train['text'].progress_apply(lambda x: score(model53, x)[0])\n","liar_valid['flair_bert_prob'], liar_valid['flair_bert_class']  = liar_valid['text'].progress_apply(lambda x: score(model53, x)[1]), liar_valid['text'].progress_apply(lambda x: score(model53, x)[0])\n","liar_test['flair_bert_prob'], liar_test['flair_bert_class']  = liar_test['text'].progress_apply(lambda x: score(model53, x)[1]), liar_test['text'].progress_apply(lambda x: score(model53, x)[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 10240/10240 [21:32<00:00,  7.92it/s]\n","100%|██████████| 10240/10240 [21:31<00:00,  7.93it/s]\n","100%|██████████| 1284/1284 [02:41<00:00,  7.93it/s]\n","100%|██████████| 1284/1284 [02:41<00:00,  7.95it/s]\n","100%|██████████| 1267/1267 [02:42<00:00,  7.79it/s]\n","100%|██████████| 1267/1267 [02:42<00:00,  7.80it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"QWtioj9ubLZJ","colab_type":"text"},"source":["# Model Building"]},{"cell_type":"markdown","metadata":{"id":"aCabuf1kcevt","colab_type":"text"},"source":["#### Bi-LSTM"]},{"cell_type":"code","metadata":{"id":"QmycM3OObPQd","colab_type":"code","colab":{}},"source":["def run_model_sequence(model_type, dict_X_train, y_train, parameters, embeddings, input_dims, output_dims, bidirect=False):\n","  X_train = []\n","  names = []\n","  for key, value in dict_X_train.items():\n","    names.append(key)\n","    X_train.append(value)\n","  models = {}\n","  list_models = []\n","  inputs = {}\n","  dict_train = {}\n","  list_inputs = []\n","  dims = {}\n","  for i, f in enumerate(X_train):\n","    dims[i] = f.shape[1]\n","    inputs[i] = Input(shape=(dims[i],), dtype='int32', name='{}_input'.format(names[i]))\n","    dict_train['{}_input'.format(names[i])] = f\n","    list_inputs.append(inputs[i])\n","    if names[i] in ['statement', 'pos', 'dep']:\n","      x = Embedding(input_dim=input_dims[names[i]], output_dim=output_dims[names[i]], weights=[embeddings[names[i]]], \n","                    input_length=dims[i], trainable=False)(inputs[i])\n","      if model_type == 'LSTM':\n","        if bidirect:\n","          x = Bidirectional(LSTM(units=parameters['model_unit'], dropout = parameters['model_dropout'],\n","                                 activation = 'relu'))(x)\n","        else:\n","          x = LSTM(units=parameters['model_unit'], dropout = parameters['model_dropout'],\n","                   activation = 'relu')(x)\n","      elif model_type == 'GRU':\n","        if bidirect:\n","          x = Bidirectional(GRU(units=parameters['model_unit'], dropout = parameters['model_dropout'],\n","                                 activation = 'relu'))(x)\n","        else:\n","          x = GRU(units=parameters['model_unit'], dropout = parameters['model_dropout'],\n","                   activation = 'relu')(x)\n","    else:\n","      x = Dense(units=parameters['dense_unit'], activation='relu')(inputs[i])\n","    models[i] = x\n","    list_models.append(models[i])\n","\n","  if len(list_models) == 1:\n","    x = list_models[0]\n","  else:\n","    x = concatenate(list_models)\n","  main_output = Dense(units=len(dlabel), activation='softmax', name='main_output')(x)\n","  model_dl = Model(inputs=list_inputs, outputs=[main_output])\n","  model_dl.compile(optimizer=optimizers.SGD(learning_rate=parameters['eta'], clipvalue=0.3, nesterov=True), \n","                   loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","  model_dl.fit(dict_train, {'main_output': y_train}, epochs=parameters['epoch'], batch_size=parameters['batch'], verbose=0)\n","  return model_dl"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JE3RUFr8cm8c","colab_type":"text"},"source":["#### CNN"]},{"cell_type":"code","metadata":{"id":"Uns9Pd93cn_h","colab_type":"code","colab":{}},"source":["def run_model_cnn(dict_X_train, y_train, parameters, embeddings, input_dims, output_dims):\n","  X_train = []\n","  names = []\n","  for key, value in dict_X_train.items():\n","    names.append(key)\n","    X_train.append(value)\n","  models = {}\n","  list_models = []\n","  inputs = {}\n","  dict_train = {}\n","  list_inputs = []\n","  dims = {}\n","  for i, f in enumerate(X_train):\n","    dims[i] = f.shape[1]\n","    inputs[i] = Input(shape=(dims[i],), dtype='int32', name='{}_input'.format(names[i]))\n","    dict_train['{}_input'.format(names[i])] = f\n","    list_inputs.append(inputs[i])\n","    if names[i] in ['statement', 'pos', 'dep']:\n","      x = Embedding(input_dim=input_dims[names[i]], output_dim=output_dims[names[i]], weights=[embeddings[names[i]]], \n","                    input_length=dims[i], trainable=False)(inputs[i])\n","      #Multichannel convolution for statment input\n","      conv0 = Conv1D(parameters['filters'], parameters['kernels'][0])(x)\n","      conv1 = Conv1D(parameters['filters'], parameters['kernels'][1])(x)\n","      conv2 = Conv1D(parameters['filters'], parameters['kernels'][2])(x)\n","\n","      #individual GlobalMaxPooling\n","      maxpool0 = GlobalMaxPool1D()(conv0)\n","      maxpool1 = GlobalMaxPool1D()(conv1)\n","      maxpool2 = GlobalMaxPool1D()(conv2)\n","\n","      #Statement convolution input\n","      conv_in = concatenate([maxpool0, maxpool1, maxpool2])\n","      conv_in = Dropout(parameters['model_dropout'])(conv_in)\n","      x = Dense(units=parameters['dense_model_unit'], activation='relu')(conv_in)\n","    else:\n","      x = Dense(units=parameters['dense_unit'], activation='relu')(inputs[i])\n","    models[i] = x\n","    list_models.append(models[i])\n","\n","  if len(list_models) == 1:\n","    x = list_models[0]\n","  else:\n","    x = concatenate(list_models)\n","  main_output = Dense(units=len(dlabel), activation='softmax', name='main_output')(x)\n","  model_dl = Model(inputs=list_inputs, outputs=[main_output])\n","  model_dl.compile(optimizer=optimizers.SGD(learning_rate=parameters['eta'], clipvalue=0.3, nesterov=True), \n","                   loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","  model_dl.fit(dict_train, {'main_output': y_train}, epochs=parameters['epoch'], batch_size=parameters['batch'], verbose=0)\n","  return model_dl"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X-QxEI6_cq9u","colab_type":"text"},"source":["#### Hybrid CNN-BiLSTM"]},{"cell_type":"code","metadata":{"id":"BJFPmAf-ctVs","colab_type":"code","colab":{}},"source":["def run_model_hybrid(dict_X_train, y_train, parameters, embeddings, input_dims, output_dims):\n","  cols = ['subject', 'speaker', 'job', 'state', 'party', 'venue']\n","  X_train = []\n","  names = []\n","  for key, value in dict_X_train.items():\n","    names.append(key)\n","    X_train.append(value)\n","  #models = {}\n","  list_models = []\n","  list_embeddings = []\n","  inputs = {}\n","  dict_train = {}\n","  list_inputs = []\n","  dims = {}\n","  for i, f in enumerate(X_train):\n","    dims[i] = f.shape[1]\n","    inputs[i] = Input(shape=(dims[i],), dtype='int32', name='{}_input'.format(names[i]))\n","    dict_train['{}_input'.format(names[i])] = f\n","    list_inputs.append(inputs[i])\n","    if names[i] in ['statement']:\n","      x = Embedding(input_dim=input_dims[names[i]], output_dim=parameters['dimensions'], weights=[embeddings[names[i]]], \n","                    input_length=dims[i], trainable=False)(inputs[i])\n","      #Multichannel convolution for statment input\n","      conv00 = Conv1D(parameters['cnn_filters'], parameters['cnn_kernels'][0])(x)\n","      conv01 = Conv1D(parameters['cnn_filters'], parameters['cnn_kernels'][1])(x)\n","      conv02 = Conv1D(parameters['cnn_filters'], parameters['cnn_kernels'][2])(x)\n","\n","      #individual GlobalMaxPooling\n","      maxpool00 = GlobalMaxPool1D()(conv00)\n","      maxpool01 = GlobalMaxPool1D()(conv01)\n","      maxpool02 = GlobalMaxPool1D()(conv02)\n","\n","      #Statement convolution input\n","      x = concatenate([maxpool00, maxpool01, maxpool02])\n","      list_models.append(x)\n","    elif names[i] in ['dep', 'pos', 'meta']:\n","      if names[i] == 'meta':\n","        input_split = Lambda(lambda x: split(x, 6, axis=1))(inputs[i])\n","        for j, c in enumerate(cols):\n","          vocab_dim = len(ts[j].word_index) + 1\n","          x = Embedding(input_dim=vocab_dim, output_dim=parameters['dimensions'])(input_split[j])\n","          list_embeddings.append(x)\n","      else:\n","        x = Embedding(input_dim=input_dims[names[i]], output_dim=parameters['dimensions'], \n","                    input_length=dims[i], trainable=False)(inputs[i])\n","        list_embeddings.append(x)\n","    else:\n","      x = Dense(units=parameters['dense_unit'], activation='relu')(inputs[i])\n","      list_models.append(x)\n","\n","  if 'dep' in names or 'pos' in names or 'meta' in names:\n","    if len(list_embeddings) == 1:\n","      x = list_embeddings[0]\n","    else:\n","      x = concatenate(list_embeddings, axis=1)\n","    #x = concatenate(list_embeddings, axis=1)\n","    #Multichannel convolution for dep/pos/meta input\n","    conv10 = Conv1D(filters=parameters['md_filters'], kernel_size=parameters['md_kernels'][0], padding='same')(x)\n","    conv11 = Conv1D(filters=parameters['md_filters'], kernel_size=parameters['md_kernels'][1], padding='same')(x)\n","    conv12 = Conv1D(filters=parameters['md_filters'], kernel_size=parameters['md_kernels'][2], padding='same')(x)\n","\n","    #individual GlobalMaxPooling\n","    maxpool10 = GlobalMaxPool1D()(conv10)\n","    maxpool11 = GlobalMaxPool1D()(conv11)\n","    maxpool12 = GlobalMaxPool1D()(conv12)\n","\n","    x = concatenate([maxpool10, maxpool11, maxpool12])\n","    x_reshape = Reshape((x.shape[1], 1))(x)\n","    x = Bidirectional(LSTM(parameters['lstm_unit'], return_sequences=False))(x_reshape)\n","    list_models.append(x)\n","\n","  if len(list_models) == 1:\n","    x = list_models[0]\n","  else:\n","    x = concatenate(list_models)\n","  dropout_layer = Dropout(parameters['final_dropout'])(x)\n","  main_output = Dense(units=len(dlabel), activation='softmax', name='main_output')(dropout_layer)\n","  model_dl = Model(inputs=list_inputs, outputs=[main_output])\n","  model_dl.compile(optimizer=optimizers.SGD(learning_rate=parameters['eta'], clipvalue=0.3, nesterov=True), \n","                   loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","  model_dl.fit(dict_train, {'main_output': y_train}, epochs=parameters['epoch'], batch_size=parameters['batch'], verbose=0)\n","  return model_dl  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CFY_3_xicv1C","colab_type":"text"},"source":["#### k-fold Cross Validation"]},{"cell_type":"code","metadata":{"id":"tFuhRARxcygq","colab_type":"code","colab":{}},"source":["def run_cv(model_type, dict_features_cv, dict_features_test, y_cv, y_test, parameters, embeddings, input_dims, output_dims, bidirect=False):\n","  if bidirect:\n","    print('Bi-'+model_type)\n","  else:\n","    print(model_type)\n","  \n","  for j in range(len(dict_features_cv)):\n","    print('features:')\n","    dict_X_cv = dict_features_cv[j]\n","    X_cv = dict_X_cv[next(iter(dict_X_cv))]\n","    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n","    cv_scores = []\n","    names = []\n","    for k in dict_X_cv.keys():\n","      print(k)\n","      names.append(k)\n","    \n","    #run k-fold CV\n","    for train, val in kfold.split(X_cv, y_cv):\n","      #refine dictionary\n","      dict_X_train = {}\n","      dict_X_val = {}\n","      for k, v in dict_X_cv.items():\n","        dict_X_train[k] = v[train]\n","        dict_X_val[k] = v[val]\n","      \n","      y_train = to_categorical(y_cv[train], num_classes=6)\n","      y_val = to_categorical(y_cv[val], num_classes=6)\n","      if model_type in ['LSTM', 'GRU']:\n","        model_dl = run_model_sequence(model_type, dict_X_train, y_train, parameters, embeddings, input_dims, output_dims, bidirect)\n","      elif model_type == 'CNN':\n","        model_dl = run_model_cnn(dict_X_train, y_train, parameters, embeddings, input_dims, output_dims)\n","      else:\n","        model_dl = run_model_hybrid(dict_X_train, y_train, parameters, embeddings, input_dims, output_dims)\n","      #prepare validation data\n","      X_val = [v for v in dict_X_val.values()]\n","      dict_val = {}\n","      for i in range(len(X_val)):\n","        dict_val['{}_input'.format(names[i])] = X_val[i]\n","\n","      scores = model_dl.evaluate(dict_val, {'main_output': y_val}, verbose=0)\n","      print(\"%s: %.2f%%\" % (model_dl.metrics_names[1], scores[1]*100))\n","      cv_scores.append(scores[1]*100)\n","      \n","    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cv_scores), np.std(cv_scores)))\n","    #prepare test data\n","    dict_X_test = dict_features_test[j]\n","    X_test = [v for v in dict_X_test.values()]\n","    dict_test = {}\n","    for i in range(len(X_test)):\n","      dict_test['{}_input'.format(names[i])] = X_test[i]\n","    if model_type in ['LSTM', 'GRU']:\n","      model_dl_full = run_model_sequence(model_type, dict_X_cv, to_categorical(y_cv, num_classes=6), parameters, embeddings, input_dims, output_dims, bidirect)\n","    elif model_type == 'CNN':\n","      model_dl_full = run_model_cnn(dict_X_cv, to_categorical(y_cv, num_classes=6), parameters, embeddings, input_dims, output_dims)\n","    else:\n","      model_dl_full = run_model_hybrid(dict_X_cv, to_categorical(y_cv, num_classes=6), parameters, embeddings, input_dims, output_dims)\n","    #model_dl_full.summary()\n","    score_full = model_dl_full.evaluate(dict_test, {'main_output': y_test}, verbose=0)\n","    print(\"%s: %.2f%%\" % (model_dl_full.metrics_names[1], score_full[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWd2V31ac5ce","colab_type":"text"},"source":["# Experiments"]},{"cell_type":"code","metadata":{"id":"ve5UvM0Jc5GW","colab_type":"code","colab":{}},"source":["embeddings = {\n","    'statement': stmt_embedding,\n","    'pos': pos_embedding,\n","    'dep': dep_embedding\n","}\n","input_dims = {\n","    'statement': len(t.word_index) + 1,\n","    'pos': 10,\n","    'dep': 11\n","}\n","output_dims = {\n","    'statement': 300,\n","    'pos': 10,\n","    'dep': 11,\n","    'meta': 300\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6vloptNpc-AH","colab_type":"code","colab":{}},"source":["parameters_lstm = {\n","        'model_unit': 256, #64,\n","        'dense_unit': 64,\n","        'model_dropout': 0.5, #0.2,\n","        'eta': 0.01,\n","        'epoch': 30,\n","        'batch': 32\n","}\n","parameters_cnn = {\n","        'kernels': [7, 7, 7], \n","        'filters': 256, \n","        'dense_model_unit': 256, #64,\n","        'dense_unit': 64,\n","        'model_dropout': 0.5, #0.2,\n","        'eta': 0.01,\n","        'epoch': 30,\n","        'batch': 32\n","}\n","parameters_hybrid = {\n","        'dimensions': 300,\n","        'cnn_kernels': [6, 7, 8], #[7, 7, 7], \n","        'cnn_filters': 256, #256, \n","        'md_kernels': [6, 7, 8],\n","        'md_filters': 256,\n","        'lstm_unit': 64, #256,\n","        'dense_unit': 64,\n","        'final_dropout': 0.5, #0.2,\n","        'eta': 0.01,\n","        'epoch': 30,\n","        'batch': 32 #32\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nBXcgwy3dFg-","colab_type":"code","colab":{}},"source":["dict_features_cv = [{'statement': stmt_cv},\n","                    {'statement': stmt_cv, 'dep': dep_cv},\n","                    {'statement': stmt_cv, 'pos': pos_cv},\n","                    {'statement': stmt_cv, 'meta': meta_cv},\n","                    {'statement': stmt_cv, 'svm': svm_cv},\n","                    {'statement': stmt_cv, 'bert': flair_bert_cv},\n","                    {'statement': stmt_cv, 'dep': dep_cv, 'pos': pos_cv},\n","                    {'statement': stmt_cv, 'dep': dep_cv, 'pos': pos_cv, 'meta': meta_cv},\n","                    {'statement': stmt_cv, 'dep': dep_cv, 'pos': pos_cv, 'meta': meta_cv, 'svm': svm_cv},\n","                    {'statement': stmt_cv, 'dep': dep_cv, 'pos': pos_cv, 'meta': meta_cv, 'bert': flair_bert_cv}]\n","\n","dict_features_test = [{'statement': stmt_test},\n","                      {'statement': stmt_test, 'dep': dep_test},\n","                      {'statement': stmt_test, 'pos': pos_test},\n","                      {'statement': stmt_test, 'meta': meta_test},\n","                      {'statement': stmt_test, 'svm': svm_test},\n","                      {'statement': stmt_test, 'bert': flair_bert_test},\n","                      {'statement': stmt_test, 'dep': dep_test, 'pos': pos_test},\n","                      {'statement': stmt_test, 'dep': dep_test, 'pos': pos_test, 'meta': meta_test},\n","                      {'statement': stmt_test, 'dep': dep_test, 'pos': pos_test, 'meta': meta_test, 'svm': svm_test},\n","                      {'statement': stmt_test, 'dep': dep_test, 'pos': pos_test, 'meta': meta_test, 'bert': flair_bert_test}]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O2noXWW6dHDJ","colab_type":"text"},"source":["#### Bi-LSTM"]},{"cell_type":"code","metadata":{"id":"TVSoEzgcdRLE","colab_type":"code","colab":{}},"source":["run_cv('LSTM', dict_features_cv, dict_features_test, y_cv, y_test, parameters_lstm, embeddings, input_dims, output_dims, bidirect=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SvHNpiMGdRyL","colab_type":"text"},"source":["#### CNN"]},{"cell_type":"code","metadata":{"id":"V287LIcOdSm0","colab_type":"code","colab":{}},"source":["run_cv('CNN', dict_features_cv, dict_features_test, y_cv, y_test, parameters_cnn, embeddings, input_dims, output_dims)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CAl49IyzdTVB","colab_type":"text"},"source":["#### Hybrid CNN-BiLSTM"]},{"cell_type":"code","metadata":{"id":"lA1fXtwcdVmt","colab_type":"code","colab":{}},"source":["run_cv('Hybrid', dict_features_cv, dict_features_test, y_cv, y_test, parameters_hybrid, embeddings, input_dims, output_dims)"],"execution_count":0,"outputs":[]}]}